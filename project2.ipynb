{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import os \n",
    "import requests \n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "import time\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b405c3dc-ca27-4bb0-ada5-357f8531f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_GEMINI = 'gemini-1.5-flash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd613d70-fb1a-4464-840d-92bf4a977976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AIzaSyAG\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else: \n",
    "    print(\"OpenAI API Key not set\")\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "655cfb89-83c2-4f39-b9b1-6b9c5d3536b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to LLM Models\n",
    "\n",
    "openai = OpenAI()\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c02517-3568-46c0-b012-f918070707a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompts\n",
    "\n",
    "system_prompt = \"You are a code assistant helping a beginner software development student.\"\n",
    "system_prompt += \"Your explanations should be clear, structured, and beginner-friendly.\"\n",
    "system_prompt += \"If user ask any unrelevant questions, you will say you don't know and you can only answer to coding questions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2504dd-1c6c-4bbb-b1f6-072b47667913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function chat\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    # stream OpenAI \n",
    "    stream = openai.chat.completions.create(model=MODEL_GPT, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fbc8d5-31c6-4dd2-9e2c-8726596da57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradio test simple UI\n",
    "\n",
    "gr.ChatInterface(fn=stream_gemini, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a078595-e1b7-4e71-83ee-b732d1e44bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream model gpt\n",
    "\n",
    "def stream_gpt(prompt, history=[]):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n",
    "\n",
    "    talker(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f4ce29b-fb11-4d6c-8db8-6b0e4a8c8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream model llama\n",
    "\n",
    "def stream_llama(prompt, history=[]):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    stream = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk[\"message\"].get(\"content\", \"\")\n",
    "        yield result \n",
    "\n",
    "    talker(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5a41e4a-d5e7-4965-b612-f5c849498892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/1q/nyt9ydwd0j72jcxlq0jbp8300000gn/T/tmpk9b3hnar.wav':\n",
      "  Duration: 00:00:06.98, bitrate: 384 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, 1 channels, s16, 384 kb/s\n",
      "   6.88 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# stream model gemini\n",
    "\n",
    "def transform_history(history):\n",
    "    new_history = []\n",
    "    for chat in history:\n",
    "        new_history.append({\"parts\": [{\"text\": chat[0]}], \"role\": \"user\"})\n",
    "        new_history.append({\"parts\": [{\"text\": chat[1]}], \"role\": \"model\"})\n",
    "    return new_history\n",
    "\n",
    "def stream_gemini(prompt, history=[]):\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=MODEL_GEMINI,\n",
    "        system_instruction=system_prompt,\n",
    "        safety_settings=[\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chat = model.start_chat(history=transform_history(history))  # Set up chat with history\n",
    "\n",
    "    response = chat.send_message(prompt)\n",
    "    response.resolve()  # Ensure we get the full response before streaming\n",
    "\n",
    "    # Each character of the answer is displayed\n",
    "    for i in range(len(response.text)):\n",
    "        time.sleep(0.01)\n",
    "        yield response.text[: i+1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756ebbb0-0234-401c-9928-297eded62fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose llm model\n",
    "\n",
    "def stream_llm_model(prompt, history, model):\n",
    "    if model == \"GPT\":\n",
    "        result = stream_gpt(prompt, history=[])\n",
    "    elif model == \"Gemini\":\n",
    "        result = stream_gemini(prompt, history=[])\n",
    "    elif model == \"Llama\":\n",
    "        result = stream_llama(prompt, history=[])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60f23cb-9cdd-461a-8eb2-072b89bd5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add audio\n",
    "\n",
    "def talker(message):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"onyx\",\n",
    "        input=message\n",
    "    )\n",
    "\n",
    "    audio_stream = BytesIO(response.content)\n",
    "    audio = AudioSegment.from_file(audio_stream, format=\"mp3\")\n",
    "    play(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14f1860f-534b-4ec5-943e-ad02ac2e5ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/1q/nyt9ydwd0j72jcxlq0jbp8300000gn/T/tmpjmp8h1x1.wav':\n",
      "  Duration: 00:00:01.70, bitrate: 384 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, 1 channels, s16, 384 kb/s\n",
      "   1.47 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   1.58 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B "
     ]
    }
   ],
   "source": [
    "# test audio func\n",
    "\n",
    "talker(\"Hello, I am an AI assistant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "278b5c4b-5e54-4443-b511-96ddc2e9ef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* Running on public URL: https://b28107a9d90f5dfc81.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b28107a9d90f5dfc81.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI\n",
    "\n",
    "chat = gr.ChatInterface(\n",
    "    fn=stream_llm_model,\n",
    "    additional_inputs=[\n",
    "        gr.Dropdown([\"GPT\", \"Gemini\", \"Llama\"], label=\"Select model\")\n",
    "    ],\n",
    "    title=\"AI Models Coding Assistant\",\n",
    "    description=\"Select a model and start chatting about coding question.\",\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "chat.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8994809-e955-4814-8215-1634ec6e30c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
